{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590c469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory\n",
    "from langdetect.lang_detect_exception import LangDetectException\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "lang_dect = {\n",
    "    \"ar\": \"arb_Arab\",\n",
    "    \"en\": \"eng_Latn\",\n",
    "    \"ru\": \"rus_Cyrl\",\n",
    "    \"zn-ch\": \"zho_Hans\",\n",
    "    \"fr\": \"fra_Latn\"\n",
    "}\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "        return lang_dect[language]\n",
    "    except LangDetectException:\n",
    "        return \"Could not detect language\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02272af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "post = input(\"Enter the text to be summarized:\")\n",
    "src_lang = detect_language(post)\n",
    "tgt_lang = input(\"Enter a language to have the summary in (english, arabic, russian, mandarin, french):\")\n",
    "\n",
    "lang_tgt = {\n",
    "    \"arabic\": \"arb_Arab\",\n",
    "    \"english\": \"eng_Latn\",\n",
    "    \"russian\": \"rus_Cyrl\",\n",
    "    \"mandarin\": \"zho_Hans\",\n",
    "    \"french\": \"fra_Latn\"\n",
    "}\n",
    "\n",
    "tgt_lang = lang_tgt[tgt_lang.lower()]\n",
    "\n",
    "if src_lang != 'eng_Latn':\n",
    "    inputs = tokenizer(post, return_tensors=\"pt\", src_lang=src_lang)\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id['eng_Latn'])\n",
    "    post = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839e9884",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./bart-large-final\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "inputs = tokenizer(post, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "summary_ids = model.generate(**inputs, max_length=64, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83232ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from better_profanity import profanity\n",
    "import re\n",
    "\n",
    "profanity.load_censor_words()\n",
    "\n",
    "def custom_censor(text, level=\"full\"):\n",
    "    censored_text = text\n",
    "    bad_words = profanity.CENSOR_WORDSET\n",
    "\n",
    "    for word in bad_words:\n",
    "        pattern = re.compile(rf\"\\b{re.escape(word)}\\b\", re.IGNORECASE)\n",
    "        matches = pattern.findall(text)\n",
    "\n",
    "        for match in matches:\n",
    "            if level.lower() == \"none\":\n",
    "                replacement = match\n",
    "            elif level.lower() == \"partial\":\n",
    "                if len(match) <= 2:\n",
    "                    replacement = \"*\" * len(match)\n",
    "                else:\n",
    "                    replacement = match[0] + \"*\" * (len(match) - 2) + match[-1]\n",
    "            elif level.lower() == \"full\":\n",
    "                replacement = \"*\" * len(match)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid censorship level. Choose from 'none', 'partial', or 'full'.\")\n",
    "\n",
    "            censored_text = re.sub(rf\"\\b{re.escape(match)}\\b\", replacement, censored_text, flags=re.IGNORECASE)\n",
    "\n",
    "    return censored_text\n",
    "\n",
    "level = input(\"Enter your level of censorship (full, partial, none):\")\n",
    "\n",
    "summary = custom_censor(summary, level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26df6b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tgt_lang != 'eng_Latn':\n",
    "    inputs = tokenizer(summary, return_tensors=\"pt\", src_lang='eng_Latn')\n",
    "    generated_tokens = model.generate(**inputs, forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang])\n",
    "    summary = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0110ac05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary: \", summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
